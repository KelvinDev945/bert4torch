{
  "config": "BF16 + Compile + Normuon + FlashAttn-FA2 + QKNorm + FP8-LMHead + AsyncData",
  "error": "Traceback (most recent call last):\n  File \"/home/kelvin/bert4torch/examples/pretrain_bert_fast.py\", line 262, in <module>\n    results = main(args)\n  File \"/home/kelvin/bert4torch/examples/pretrain_bert_fast.py\", line 190, in main\n    loss.backward()\n  File \"/home/kelvin/.pyenv/versions/3.10.18/lib/python3.10/site-packages/torch/_tensor.py\", line 647, in backward\n    torch.autograd.backward(\n  File \"/home/kelvin/.pyenv/versions/3.10.18/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 354, in backward\n    _engine_run_backward(\n  File \"/home/kelvin/.pyenv/versions/3.10.18/lib/python3.10/site-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File \"/home/kelvin/bert4torch/bert4torch/bert4torch/models.py\", line 33, in forward\n    outputs = self.apply_embeddings(inputs)\n  File \"/home/kelvin/bert4torch/bert4torch/bert4torch/models.py\", line 106, in apply_embeddings\n    x = self.emb_norm(x)\n  File \"/home/kelvin/bert4torch/bert4torch/bert4torch/layers.py\", line 120, in forward\n    x = self.weight * x + self.bias. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.\n",
  "success": false,
  "elapsed_time": 23.480722665786743
}
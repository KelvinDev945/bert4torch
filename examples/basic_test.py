"""åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
import torch
import sys
sys.path.insert(0, '..')

from bert4torch.models import BERT, RoFormer, GPT, T5
from bert4torch.layers import MultiHeadAttention, FeedForward


def test_layers():
    """æµ‹è¯•åŸºç¡€å±‚"""
    print("=" * 60)
    print("æµ‹è¯•åŸºç¡€å±‚")
    print("=" * 60)

    batch_size, seq_len, hidden_size = 2, 10, 768
    x = torch.randn(batch_size, seq_len, hidden_size)

    # Test MultiHeadAttention
    print("\n[1] MultiHeadAttention")
    attn = MultiHeadAttention(hidden_size=768, num_heads=12)
    output = attn(x)
    print(f"  Input: {x.shape} -> Output: {output.shape}")
    assert output.shape == x.shape

    # Test FeedForward
    print("\n[2] FeedForward")
    ffn = FeedForward(hidden_size=768, intermediate_size=3072)
    output = ffn(x)
    print(f"  Input: {x.shape} -> Output: {output.shape}")
    assert output.shape == x.shape

    print("\nâœ“ åŸºç¡€å±‚æµ‹è¯•é€šè¿‡!")


def test_bert():
    """æµ‹è¯•BERTæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•BERTæ¨¡å‹")
    print("=" * 60)

    batch_size, seq_len = 2, 128
    vocab_size, hidden_size = 21128, 768

    model = BERT(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )

    # æµ‹è¯•å‰å‘ä¼ æ’­
    token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    segment_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)

    output = model(token_ids, segment_ids)
    print(f"\nBERTè¾“å‡º: {output.shape}")
    print(f"æœŸæœ›: [{batch_size}, {seq_len}, {hidden_size}]")
    assert output.shape == (batch_size, seq_len, hidden_size)

    print("\nâœ“ BERTæ¨¡å‹æµ‹è¯•é€šè¿‡!")


def test_roformer():
    """æµ‹è¯•RoFormeræ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•RoFormeræ¨¡å‹")
    print("=" * 60)

    batch_size, seq_len = 2, 128
    vocab_size, hidden_size = 21128, 768

    model = RoFormer(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )

    token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    output = model(token_ids)

    print(f"\nRoFormerè¾“å‡º: {output.shape}")
    assert output.shape == (batch_size, seq_len, hidden_size)

    print("\nâœ“ RoFormeræ¨¡å‹æµ‹è¯•é€šè¿‡!")


def test_gpt():
    """æµ‹è¯•GPTæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•GPTæ¨¡å‹")
    print("=" * 60)

    batch_size, seq_len = 2, 128
    vocab_size, hidden_size = 50257, 768

    model = GPT(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )

    token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    output = model(token_ids)

    print(f"\nGPTè¾“å‡º: {output.shape}")
    assert output.shape == (batch_size, seq_len, hidden_size)

    print("\nâœ“ GPTæ¨¡å‹æµ‹è¯•é€šè¿‡!")


def test_t5():
    """æµ‹è¯•T5æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•T5æ¨¡å‹")
    print("=" * 60)

    batch_size, seq_len = 2, 128
    vocab_size, hidden_size = 32128, 768

    model = T5(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )

    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    decoder_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

    output = model(input_ids, decoder_input_ids)

    print(f"\nT5è¾“å‡º: {output.shape}")
    print(f"æœŸæœ›: [{batch_size}, {seq_len}, {vocab_size}]")
    assert output.shape == (batch_size, seq_len, vocab_size)

    print("\nâœ“ T5æ¨¡å‹æµ‹è¯•é€šè¿‡!")


if __name__ == '__main__':
    test_layers()
    test_bert()
    test_roformer()
    test_gpt()
    test_t5()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 60)

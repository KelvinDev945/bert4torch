"""Seq2Seq ç¤ºä¾‹ - T5 è‡ªåŠ¨æ ‡é¢˜ç”Ÿæˆ"""
import torch
import torch.nn as nn
import sys
sys.path.insert(0, '..')

from bert4torch.models import T5
from bert4torch.snippets import sequence_padding, AutoRegressiveDecoder


class T5Seq2Seq(nn.Module):
    """T5 Seq2Seq æ¨¡å‹"""
    def __init__(self, vocab_size):
        super().__init__()
        self.t5 = T5(
            vocab_size=vocab_size,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072
        )

    def forward(self, input_ids, decoder_input_ids):
        return self.t5(input_ids, decoder_input_ids)


class T5Generator(AutoRegressiveDecoder):
    """T5 ç”Ÿæˆå™¨"""
    def __init__(self, model, tokenizer, start_id, end_id, maxlen=64, device='cpu'):
        super().__init__(start_id, end_id, maxlen, device=device)
        self.model = model
        self.tokenizer = tokenizer
        self.model.eval()

    @torch.no_grad()
    def predict(self, inputs, output_ids):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªtoken"""
        encoder_outputs = self.model.t5.encode(inputs)
        decoder_outputs = self.model.t5.decode(output_ids, encoder_outputs)
        logits = self.model.t5.lm_head(decoder_outputs)
        return logits[:, -1, :]


def demo_title_generation():
    """æ¼”ç¤ºæ ‡é¢˜ç”Ÿæˆ"""
    print("=" * 60)
    print("T5 Seq2Seq ç¤ºä¾‹ - è‡ªåŠ¨æ ‡é¢˜ç”Ÿæˆ")
    print("=" * 60)

    # æ¨¡å‹å‚æ•°
    vocab_size = 32128

    # åˆ›å»ºæ¨¡å‹
    model = T5Seq2Seq(vocab_size)
    model.eval()

    # æ¨¡æ‹Ÿæ•°æ®
    # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™äº›åº”è¯¥æ¥è‡ª tokenizer
    # è¾“å…¥ï¼šæ–°é—»æ­£æ–‡
    # è¾“å‡ºï¼šæ–°é—»æ ‡é¢˜
    articles = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ¸¸ç©ã€‚",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œæ·±åº¦å­¦ä¹ æˆä¸ºçƒ­é—¨ç ”ç©¶æ–¹å‘ã€‚",
        "æ–°èƒ½æºæ±½è½¦é”€é‡æŒç»­å¢é•¿ï¼Œå¸‚åœºå‰æ™¯å¹¿é˜”ã€‚"
    ]

    # æ¨¡æ‹Ÿ token ids
    input_ids = [
        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        [1, 11, 12, 13, 14, 15, 16, 17, 18],
        [1, 19, 20, 21, 22, 23, 24, 25]
    ]

    print("\nç¤ºä¾‹æ•°æ®:")
    for i, article in enumerate(articles):
        print(f"\n[{i+1}] åŸæ–‡: {article[:30]}...")

    # Padding
    input_ids = sequence_padding(input_ids)
    input_ids = torch.tensor(input_ids)

    print(f"\nè¾“å…¥shape: {input_ids.shape}")

    # æ¨¡æ‹Ÿç”Ÿæˆï¼ˆå®é™…éœ€è¦å®ç°å®Œæ•´çš„è§£ç é€»è¾‘ï¼‰
    decoder_input_ids = torch.ones((input_ids.shape[0], 1), dtype=torch.long)  # start token

    with torch.no_grad():
        outputs = model(input_ids, decoder_input_ids)

    print(f"è¾“å‡ºshape: {outputs.shape}")

    # æ¨¡æ‹Ÿç”Ÿæˆçš„æ ‡é¢˜
    generated_titles = [
        "å¤©æ°”æ™´å¥½é€‚åˆå‡ºæ¸¸",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ",
        "æ–°èƒ½æºæ±½è½¦å¸‚åœºå‰æ™¯å¥½"
    ]

    print("\nç”Ÿæˆçš„æ ‡é¢˜:")
    for i, title in enumerate(generated_titles):
        print(f"  [{i+1}] {title}")

    print("\nâœ“ æ ‡é¢˜ç”Ÿæˆæ¼”ç¤ºå®Œæˆ!")


def demo_translation():
    """æ¼”ç¤ºæœºå™¨ç¿»è¯‘"""
    print("\n" + "=" * 60)
    print("T5 Seq2Seq ç¤ºä¾‹ - æœºå™¨ç¿»è¯‘")
    print("=" * 60)

    vocab_size = 32128

    # åˆ›å»ºæ¨¡å‹
    model = T5Seq2Seq(vocab_size)
    model.eval()

    # ç¿»è¯‘ä»»åŠ¡ç¤ºä¾‹
    examples = [
        {
            "source": "translate English to Chinese: Hello, how are you?",
            "target": "ä½ å¥½ï¼Œä½ æ€ä¹ˆæ ·ï¼Ÿ"
        },
        {
            "source": "translate English to Chinese: I love machine learning.",
            "target": "æˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ ã€‚"
        }
    ]

    print("\nç¿»è¯‘ç¤ºä¾‹:")
    for i, example in enumerate(examples):
        print(f"\n[{i+1}]")
        print(f"  è¾“å…¥: {example['source']}")
        print(f"  è¾“å‡º: {example['target']}")

    print("\nè¯´æ˜:")
    print("  T5 ä½¿ç”¨ç»Ÿä¸€çš„ text-to-text æ ¼å¼")
    print("  é€šè¿‡æ·»åŠ å‰ç¼€æ¥æŒ‡å®šä»»åŠ¡ç±»å‹")
    print("  æ”¯æŒå¤šç§ NLP ä»»åŠ¡ï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç­‰ï¼‰")

    print("\nâœ“ æœºå™¨ç¿»è¯‘æ¼”ç¤ºå®Œæˆ!")


def demo_summarization():
    """æ¼”ç¤ºæ–‡æœ¬æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("T5 Seq2Seq ç¤ºä¾‹ - æ–‡æœ¬æ‘˜è¦")
    print("=" * 60)

    vocab_size = 32128
    model = T5Seq2Seq(vocab_size)
    model.eval()

    # æ‘˜è¦ä»»åŠ¡ç¤ºä¾‹
    examples = [
        {
            "document": "summarize: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ"
                       "å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚"
                       "è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚",
            "summary": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦åˆ†æ”¯ï¼Œç ”ç©¶æ™ºèƒ½æœºå™¨ã€‚"
        }
    ]

    print("\næ‘˜è¦ç¤ºä¾‹:")
    for i, example in enumerate(examples):
        print(f"\n[{i+1}]")
        print(f"  åŸæ–‡: {example['document'][:50]}...")
        print(f"  æ‘˜è¦: {example['summary']}")

    print("\næ‘˜è¦ç­–ç•¥:")
    print("  - æŠ½å–å¼: é€‰æ‹©åŸæ–‡ä¸­çš„å…³é”®å¥å­")
    print("  - ç”Ÿæˆå¼: ç”Ÿæˆæ–°çš„æ¦‚æ‹¬æ€§å¥å­")
    print("  - T5 å±äºç”Ÿæˆå¼æ‘˜è¦æ¨¡å‹")

    print("\nâœ“ æ–‡æœ¬æ‘˜è¦æ¼”ç¤ºå®Œæˆ!")


def demo_training_pipeline():
    """æ¼”ç¤ºè®­ç»ƒæµç¨‹"""
    print("\n" + "=" * 60)
    print("T5 è®­ç»ƒæµç¨‹ç¤ºä¾‹")
    print("=" * 60)

    print("\nè®­ç»ƒæ­¥éª¤:")
    print("""
1. æ•°æ®å‡†å¤‡
   - æ”¶é›†å¹³è¡Œè¯­æ–™ï¼ˆæºæ–‡æœ¬ - ç›®æ ‡æ–‡æœ¬å¯¹ï¼‰
   - æ·»åŠ ä»»åŠ¡å‰ç¼€ï¼ˆå¦‚ "summarize:", "translate:"ï¼‰
   - åˆ†è¯å’Œç¼–ç 

2. æ¨¡å‹è®­ç»ƒ
   - è¾“å…¥ï¼šæºæ–‡æœ¬çš„ token ids
   - è¾“å‡ºï¼šç›®æ ‡æ–‡æœ¬çš„ token ids
   - æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±

3. ç”Ÿæˆç­–ç•¥
   - Greedy Search: æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
   - Beam Search: ä¿ç•™å¤šä¸ªå€™é€‰åºåˆ—
   - Sampling: æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·

4. è¯„ä¼°æŒ‡æ ‡
   - BLEU: æœºå™¨ç¿»è¯‘
   - ROUGE: æ–‡æœ¬æ‘˜è¦
   - Perplexity: è¯­è¨€æ¨¡å‹å›°æƒ‘åº¦
    """)

    print("\nç¤ºä¾‹ä»£ç :")
    print("""
# è®­ç»ƒ
for epoch in range(num_epochs):
    for batch in dataloader:
        input_ids, decoder_input_ids, labels = batch

        # å‰å‘ä¼ æ’­
        logits = model(input_ids, decoder_input_ids)

        # è®¡ç®—æŸå¤±
        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# ç”Ÿæˆ
input_text = "summarize: ..."
input_ids = tokenizer.encode(input_text)

# è‡ªå›å½’ç”Ÿæˆ
decoder_input_ids = [start_token]
for _ in range(max_length):
    logits = model(input_ids, decoder_input_ids)
    next_token = torch.argmax(logits[:, -1, :])
    decoder_input_ids.append(next_token)
    if next_token == end_token:
        break
    """)


def demo_task_prefixes():
    """æ¼”ç¤º T5 çš„ä»»åŠ¡å‰ç¼€"""
    print("\n" + "=" * 60)
    print("T5 ä»»åŠ¡å‰ç¼€ç¤ºä¾‹")
    print("=" * 60)

    tasks = [
        ("ç¿»è¯‘", "translate English to Chinese: Hello"),
        ("æ‘˜è¦", "summarize: è¿™æ˜¯ä¸€ç¯‡å¾ˆé•¿çš„æ–‡ç« ..."),
        ("é—®ç­”", "question: What is AI? context: AI is..."),
        ("åˆ†ç±»", "cola sentence: This sentence is grammatical."),
        ("ç›¸ä¼¼åº¦", "stsb sentence1: I like cats. sentence2: I love dogs."),
        ("è•´å«", "mnli premise: ... hypothesis: ..."),
    ]

    print("\nT5 æ”¯æŒçš„ä»»åŠ¡ç±»å‹:")
    for task_name, example in tasks:
        print(f"\n  {task_name}:")
        print(f"    {example}")

    print("\nç‰¹ç‚¹:")
    print("  â€¢ ç»Ÿä¸€çš„ text-to-text æ¡†æ¶")
    print("  â€¢ é€šè¿‡å‰ç¼€åŒºåˆ†ä¸åŒä»»åŠ¡")
    print("  â€¢ å¯ä»¥è½»æ¾æ‰©å±•åˆ°æ–°ä»»åŠ¡")
    print("  â€¢ å¤šä»»åŠ¡å­¦ä¹ ")


if __name__ == '__main__':
    demo_title_generation()
    demo_translation()
    demo_summarization()
    demo_training_pipeline()
    demo_task_prefixes()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)

    print("\næ€»ç»“:")
    print("  â€¢ T5 æ˜¯å¼ºå¤§çš„ Seq2Seq æ¨¡å‹")
    print("  â€¢ æ”¯æŒå¤šç§ NLP ä»»åŠ¡")
    print("  â€¢ ä½¿ç”¨ text-to-text ç»Ÿä¸€æ ¼å¼")
    print("  â€¢ éœ€è¦å¤§é‡å¹³è¡Œè¯­æ–™è®­ç»ƒ")

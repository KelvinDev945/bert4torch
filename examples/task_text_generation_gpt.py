"""æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ - GPT"""
import torch
import torch.nn as nn
import sys
sys.path.insert(0, '..')

from bert4torch.models import GPT
from bert4torch.snippets import AutoRegressiveDecoder


class TextGenerator(AutoRegressiveDecoder):
    """GPTæ–‡æœ¬ç”Ÿæˆå™¨"""
    def __init__(self, model, start_id, end_id, maxlen=128, device='cpu'):
        super().__init__(start_id, end_id, maxlen, device=device)
        self.model = model
        self.model.eval()

    @torch.no_grad()
    def predict(self, inputs, output_ids):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªtoken"""
        token_ids = output_ids
        outputs = self.model(token_ids)
        # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
        return outputs[:, -1, :]


def demo_greedy_search():
    """æ¼”ç¤ºè´ªå¿ƒæœç´¢"""
    print("=" * 60)
    print("GPT æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ - è´ªå¿ƒæœç´¢")
    print("=" * 60)

    # æ¨¡å‹å‚æ•°
    vocab_size = 50257
    batch_size = 1

    # åˆ›å»ºæ¨¡å‹
    model = GPT(
        vocab_size=vocab_size,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )
    model.eval()

    # æ¨¡æ‹Ÿè¾“å…¥æç¤ºè¯
    # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™åº”è¯¥æ¥è‡ª tokenizer
    prompt = "Once upon a time"
    prompt_ids = [1, 2345, 5678, 890]  # æ¨¡æ‹Ÿ token ids

    print(f"\nè¾“å…¥æç¤º: {prompt}")
    print(f"Prompt IDs: {prompt_ids}")

    # è´ªå¿ƒæœç´¢ç”Ÿæˆ
    token_ids = torch.tensor([prompt_ids])
    max_length = 50

    generated_ids = []
    for _ in range(max_length - len(prompt_ids)):
        outputs = model(token_ids)
        next_token_logits = outputs[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1)

        generated_ids.append(next_token.item())
        token_ids = torch.cat([token_ids, next_token.unsqueeze(0)], dim=1)

        # å¦‚æœç”Ÿæˆäº†ç»“æŸç¬¦å°±åœæ­¢
        if next_token.item() == 0:  # å‡è®¾ 0 æ˜¯ EOS
            break

    print(f"\nç”Ÿæˆçš„ token IDs: {generated_ids[:20]}...")
    print(f"ç”Ÿæˆé•¿åº¦: {len(generated_ids)}")

    print("\nâœ“ è´ªå¿ƒæœç´¢å®Œæˆ!")


def demo_beam_search():
    """æ¼”ç¤ºæŸæœç´¢"""
    print("\n" + "=" * 60)
    print("GPT æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ - æŸæœç´¢")
    print("=" * 60)

    vocab_size = 50257

    # åˆ›å»ºæ¨¡å‹
    model = GPT(
        vocab_size=vocab_size,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )

    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TextGenerator(
        model=model,
        start_id=1,
        end_id=0,
        maxlen=50,
        device='cpu'
    )

    # æ¨¡æ‹Ÿè¾“å…¥
    prompt_ids = torch.tensor([[1, 2345, 5678]])

    print("\nä½¿ç”¨æŸæœç´¢ç”Ÿæˆ (beam_size=3)...")
    # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯æ¼”ç¤ºæ¥å£ï¼Œå®é™…çš„ beam_search éœ€è¦é€‚é… GPT
    # generated = generator.beam_search(prompt_ids, topk=3)
    # print(f"ç”Ÿæˆç»“æœ: {generated}")

    print("\nâœ“ æŸæœç´¢æ¼”ç¤ºå®Œæˆ!")


def demo_sampling():
    """æ¼”ç¤ºé‡‡æ ·ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("GPT æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ - éšæœºé‡‡æ ·")
    print("=" * 60)

    vocab_size = 50257

    # åˆ›å»ºæ¨¡å‹
    model = GPT(
        vocab_size=vocab_size,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
        intermediate_size=3072
    )
    model.eval()

    # æ¨¡æ‹Ÿè¾“å…¥
    prompt_ids = [1, 2345, 5678, 890]
    token_ids = torch.tensor([prompt_ids])

    print("\né‡‡æ ·å‚æ•°:")
    print("  - Temperature: 0.8")
    print("  - Top-K: 50")
    print("  - Top-P: 0.9")

    # é‡‡æ ·ç”Ÿæˆ
    temperature = 0.8
    top_k = 50
    max_length = 30

    generated_ids = []
    for _ in range(max_length - len(prompt_ids)):
        outputs = model(token_ids)
        next_token_logits = outputs[:, -1, :] / temperature

        # Top-K è¿‡æ»¤
        if top_k > 0:
            indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
            next_token_logits[indices_to_remove] = -float('Inf')

        # é‡‡æ ·
        probs = torch.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        generated_ids.append(next_token.item())
        token_ids = torch.cat([token_ids, next_token], dim=1)

        if next_token.item() == 0:
            break

    print(f"\nç”Ÿæˆçš„ token IDs: {generated_ids}")
    print(f"ç”Ÿæˆé•¿åº¦: {len(generated_ids)}")

    print("\nâœ“ éšæœºé‡‡æ ·å®Œæˆ!")


def demo_comparison():
    """å¯¹æ¯”ä¸åŒç”Ÿæˆç­–ç•¥"""
    print("\n" + "=" * 60)
    print("ä¸åŒç”Ÿæˆç­–ç•¥å¯¹æ¯”")
    print("=" * 60)

    strategies = [
        ("è´ªå¿ƒæœç´¢", "æœ€å¿«ï¼Œä½†å¯èƒ½é‡å¤"),
        ("æŸæœç´¢", "è´¨é‡å¥½ï¼Œé€Ÿåº¦ä¸­ç­‰"),
        ("Top-K é‡‡æ ·", "å¤šæ ·æ€§é«˜ï¼Œé€Ÿåº¦å¿«"),
        ("Top-P é‡‡æ ·", "å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§"),
        ("Temperature é‡‡æ ·", "æ§åˆ¶éšæœºæ€§")
    ]

    print("\nç­–ç•¥å¯¹æ¯”:")
    for name, desc in strategies:
        print(f"  â€¢ {name:15} - {desc}")

    print("\næ¨èä½¿ç”¨åœºæ™¯:")
    print("  - çŸ­æ–‡æœ¬è¡¥å…¨: è´ªå¿ƒæœç´¢")
    print("  - é•¿æ–‡æœ¬ç”Ÿæˆ: Top-P é‡‡æ · (p=0.9)")
    print("  - åˆ›æ„å†™ä½œ: é«˜ temperature (>1.0) + Top-K")
    print("  - æ‘˜è¦ç”Ÿæˆ: æŸæœç´¢ (beam_size=4)")


if __name__ == '__main__':
    demo_greedy_search()
    demo_beam_search()
    demo_sampling()
    demo_comparison()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)

    print("\nä½¿ç”¨è¯´æ˜:")
    print("1. å®é™…ä½¿ç”¨æ—¶éœ€è¦åŠ è½½é¢„è®­ç»ƒçš„ GPT æƒé‡")
    print("2. ä½¿ç”¨ Tokenizer å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ")
    print("3. æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ç”Ÿæˆç­–ç•¥")
    print("4. è°ƒæ•´ temperatureã€top_kã€top_p å‚æ•°")
    print("\nç¤ºä¾‹ä»£ç :")
    print("""
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    prompt = "Once upon a time"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆ
    output_ids = model.generate(input_ids, max_length=50)
    text = tokenizer.decode(output_ids[0])
    """)

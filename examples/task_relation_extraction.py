"""å…³ç³»æŠ½å–ç¤ºä¾‹ - BERT + GlobalPointer"""
import torch
import torch.nn as nn
import sys
sys.path.insert(0, '..')

from bert4torch.models import BERT
from bert4torch.layers import GlobalPointer
from bert4torch.snippets import sequence_padding


class RelationExtractionModel(nn.Module):
    """å…³ç³»æŠ½å–æ¨¡å‹ï¼ˆBERT + GlobalPointerï¼‰"""
    def __init__(self, vocab_size, num_relations, head_size=64):
        super().__init__()
        self.bert = BERT(
            vocab_size=vocab_size,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072
        )
        self.dropout = nn.Dropout(0.1)
        # GlobalPointer for relation extraction
        # heads = num_relations (æ¯ç§å…³ç³»ä¸€ä¸ªå¤´)
        self.global_pointer = GlobalPointer(
            hidden_size=768,
            heads=num_relations,
            head_size=head_size,
            use_rope=True
        )

    def forward(self, token_ids, segment_ids):
        hidden = self.bert(token_ids, segment_ids)
        hidden = self.dropout(hidden)
        # scores: [batch, num_relations, seq_len, seq_len]
        scores = self.global_pointer(hidden)
        return scores


def demo_entity_pair_extraction():
    """æ¼”ç¤ºå®ä½“å¯¹æŠ½å–"""
    print("=" * 60)
    print("å…³ç³»æŠ½å–ç¤ºä¾‹ - å®ä½“å¯¹æŠ½å–")
    print("=" * 60)

    # å‚æ•°
    vocab_size = 21128
    # å…³ç³»ç±»å‹ï¼šäººç‰©-å‡ºç”Ÿåœ°ã€äººç‰©-èŒä¸šã€å…¬å¸-åˆ›å§‹äººç­‰
    relations = ['PER-BIRTH', 'PER-JOB', 'COM-FOUNDER', 'COM-LOC']
    num_relations = len(relations)

    # åˆ›å»ºæ¨¡å‹
    model = RelationExtractionModel(vocab_size, num_relations)
    model.eval()

    # æ¨¡æ‹Ÿæ•°æ®
    # æ–‡æœ¬: "é©¬äº‘åˆ›å»ºäº†é˜¿é‡Œå·´å·´å…¬å¸ï¼Œæ€»éƒ¨ä½äºæ­å·"
    # å®ä½“: é©¬äº‘(PER), é˜¿é‡Œå·´å·´(COM), æ­å·(LOC)
    # å…³ç³»: (é©¬äº‘, åˆ›å»º, é˜¿é‡Œå·´å·´), (é˜¿é‡Œå·´å·´, ä½äº, æ­å·)

    texts = [
        "é©¬äº‘åˆ›å»ºäº†é˜¿é‡Œå·´å·´å…¬å¸ï¼Œæ€»éƒ¨ä½äºæ­å·",
        "æ¯”å°”ç›–èŒ¨æ˜¯å¾®è½¯å…¬å¸çš„åˆ›å§‹äºº"
    ]

    # æ¨¡æ‹Ÿ token ids
    token_ids = [
        [101, 7722, 756, 1140, 2456, 749, 7350, 7030, 7030, 1086, 1385, 102],
        [101, 3683, 2209, 4919, 4868, 3221, 2523, 6763, 1086, 1385, 102]
    ]

    print("\nè¾“å…¥æ–‡æœ¬:")
    for i, text in enumerate(texts):
        print(f"  [{i+1}] {text}")

    # Padding
    token_ids = sequence_padding(token_ids)
    segment_ids = torch.zeros_like(torch.tensor(token_ids))
    token_ids = torch.tensor(token_ids)

    print(f"\nToken IDs shape: {token_ids.shape}")

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        scores = model(token_ids, segment_ids)

    print(f"è¾“å‡º scores shape: {scores.shape}")
    print(f"  [batch_size, num_relations, seq_len, seq_len]")

    # æ¨¡æ‹Ÿè§£ç ç»“æœ
    print("\næå–çš„å…³ç³»:")
    relations_extracted = [
        ("é©¬äº‘", "COM-FOUNDER", "é˜¿é‡Œå·´å·´", 0.95),
        ("é˜¿é‡Œå·´å·´", "COM-LOC", "æ­å·", 0.88),
    ]

    for subj, rel, obj, score in relations_extracted:
        print(f"  ({subj}, {rel}, {obj}) - ç½®ä¿¡åº¦: {score:.2f}")

    print("\nâœ“ å®ä½“å¯¹æŠ½å–å®Œæˆ!")


def demo_multilabel_classification():
    """æ¼”ç¤ºå¤šæ ‡ç­¾åˆ†ç±»"""
    print("\n" + "=" * 60)
    print("å…³ç³»æŠ½å–ç¤ºä¾‹ - å¤šæ ‡ç­¾åˆ†ç±»æ–¹å¼")
    print("=" * 60)

    print("\nGlobalPointer å·¥ä½œåŸç†:")
    print("""
1. è¾“å…¥æ–‡æœ¬ç»è¿‡ BERT ç¼–ç 
2. GlobalPointer è®¡ç®—æ¯å¯¹ä½ç½®çš„å…³ç³»å¾—åˆ†
3. å¯¹äºæ¯ç§å…³ç³»ç±»å‹ï¼Œå¾—åˆ°ä¸€ä¸ª [seq_len, seq_len] çš„çŸ©é˜µ
4. çŸ©é˜µä¸­ (i, j) è¡¨ç¤ºä»ä½ç½® i åˆ°ä½ç½® j å­˜åœ¨è¯¥å…³ç³»çš„å¾—åˆ†
5. ä½¿ç”¨é˜ˆå€¼æˆ– Top-K é€‰æ‹©æœ€å¯èƒ½çš„å…³ç³»ä¸‰å…ƒç»„
    """)

    print("\nä¼˜ç‚¹:")
    print("  â€¢ å¯ä»¥å¤„ç†å®ä½“é‡å é—®é¢˜")
    print("  â€¢ å¯ä»¥ä¸€æ¬¡æŠ½å–å¤šä¸ªå…³ç³»")
    print("  â€¢ ä½¿ç”¨ RoPE ä½ç½®ç¼–ç å¢å¼ºä½ç½®ä¿¡æ¯")
    print("  â€¢ è®­ç»ƒå’Œæ¨ç†æ•ˆç‡é«˜")

    print("\nä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”:")
    print("  ä¼ ç»Ÿæ–¹æ³•: å…ˆè¯†åˆ«å®ä½“ï¼Œå†åˆ†ç±»å…³ç³»")
    print("  GlobalPointer: ç«¯åˆ°ç«¯ç›´æ¥æŠ½å–å…³ç³»")


def demo_training():
    """æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹"""
    print("\n" + "=" * 60)
    print("å…³ç³»æŠ½å–æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)

    print("\næ•°æ®æ ‡æ³¨æ ¼å¼:")
    print("""
{
    "text": "é©¬äº‘åˆ›å»ºäº†é˜¿é‡Œå·´å·´å…¬å¸",
    "relations": [
        {
            "subject": "é©¬äº‘",
            "subject_type": "PER",
            "predicate": "åˆ›å»º",
            "object": "é˜¿é‡Œå·´å·´",
            "object_type": "COM",
            "relation_type": "COM-FOUNDER"
        }
    ]
}
    """)

    print("\næŸå¤±å‡½æ•°:")
    print("""
def multilabel_categorical_crossentropy(y_true, y_pred):
    # y_true: [batch, heads, seq_len, seq_len]
    # y_pred: [batch, heads, seq_len, seq_len]

    # å¯¹äºæ­£æ ·æœ¬ï¼Œæœ€å¤§åŒ–é¢„æµ‹åˆ†æ•°
    # å¯¹äºè´Ÿæ ·æœ¬ï¼Œæœ€å°åŒ–é¢„æµ‹åˆ†æ•°

    y_pred = (1 - 2 * y_true) * y_pred
    y_pred_pos = y_pred - (1 - y_true) * 1e12
    y_pred_neg = y_pred - y_true * 1e12

    y_pred_pos = torch.logsumexp(y_pred_pos, dim=-1)
    y_pred_neg = torch.logsumexp(y_pred_neg, dim=-1)

    loss = torch.logsumexp(torch.stack([y_pred_pos, y_pred_neg], dim=-1), dim=-1)
    return loss.mean()
    """)

    print("\nè®­ç»ƒæŠ€å·§:")
    print("  â€¢ ä½¿ç”¨å¯¹æŠ—è®­ç»ƒæå‡é²æ£’æ€§")
    print("  â€¢ æ•°æ®å¢å¼ºï¼ˆåŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ç­‰ï¼‰")
    print("  â€¢ è´Ÿé‡‡æ ·ç­–ç•¥")
    print("  â€¢ ç±»åˆ«å¹³è¡¡")


def demo_inference():
    """æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
    print("\n" + "=" * 60)
    print("å…³ç³»æŠ½å–æ¨ç†")
    print("=" * 60)

    print("\næ¨ç†æ­¥éª¤:")
    print("""
1. æ–‡æœ¬ç¼–ç 
   text = "é©¬äº‘åˆ›å»ºäº†é˜¿é‡Œå·´å·´å…¬å¸"
   token_ids = tokenizer.encode(text)

2. æ¨¡å‹é¢„æµ‹
   scores = model(token_ids)  # [1, num_relations, seq_len, seq_len]

3. è§£ç 
   for relation_id in range(num_relations):
       relation_scores = scores[0, relation_id]  # [seq_len, seq_len]

       # æ‰¾å‡ºå¾—åˆ† > é˜ˆå€¼çš„ä½ç½®å¯¹
       indices = torch.where(relation_scores > threshold)

       for i, j in zip(indices[0], indices[1]):
           subject = text[i:j+1]
           object = text[...]
           relation = relation_names[relation_id]
           print(f"({subject}, {relation}, {object})")
    """)

    print("\nåå¤„ç†:")
    print("  â€¢ å»é™¤é‡å çš„å®ä½“å¯¹")
    print("  â€¢ åˆå¹¶ç›¸ä¼¼çš„å…³ç³»")
    print("  â€¢ è§„åˆ™è¿‡æ»¤ï¼ˆå¦‚é•¿åº¦ã€ç±»å‹åŒ¹é…ï¼‰")


def demo_use_cases():
    """æ¼”ç¤ºåº”ç”¨åœºæ™¯"""
    print("\n" + "=" * 60)
    print("å…³ç³»æŠ½å–åº”ç”¨åœºæ™¯")
    print("=" * 60)

    use_cases = [
        ("çŸ¥è¯†å›¾è°±æ„å»º", "ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±"),
        ("ä¿¡æ¯æŠ½å–", "ä»æ–°é—»ã€è®ºæ–‡ä¸­æŠ½å–ç»“æ„åŒ–ä¿¡æ¯"),
        ("äº‹ä»¶æŠ½å–", "è¯†åˆ«äº‹ä»¶çš„å‚ä¸è€…ã€æ—¶é—´ã€åœ°ç‚¹ç­‰è¦ç´ "),
        ("åŒ»ç–—é¢†åŸŸ", "æŠ½å–ç–¾ç—…-ç—‡çŠ¶ã€è¯ç‰©-å‰¯ä½œç”¨ç­‰å…³ç³»"),
        ("é‡‘èé¢†åŸŸ", "æŠ½å–å…¬å¸-é«˜ç®¡ã€æŠ•èµ„-è¢«æŠ•ç­‰å…³ç³»"),
    ]

    print("\nåº”ç”¨åœºæ™¯:")
    for name, desc in use_cases:
        print(f"\n  {name}:")
        print(f"    {desc}")

    print("\næ•°æ®é›†:")
    print("  â€¢ DuIE: ç™¾åº¦å…³ç³»æŠ½å–æ•°æ®é›†")
    print("  â€¢ NYT: çº½çº¦æ—¶æŠ¥å…³ç³»æŠ½å–æ•°æ®é›†")
    print("  â€¢ WebNLG: çŸ¥è¯†å›¾è°±åˆ°æ–‡æœ¬æ•°æ®é›†")
    print("  â€¢ SemEval: è¯­ä¹‰è¯„æµ‹å…³ç³»æŠ½å–ä»»åŠ¡")


if __name__ == '__main__':
    demo_entity_pair_extraction()
    demo_multilabel_classification()
    demo_training()
    demo_inference()
    demo_use_cases()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)

    print("\næ€»ç»“:")
    print("  â€¢ GlobalPointer æ˜¯é«˜æ•ˆçš„å…³ç³»æŠ½å–æ–¹æ³•")
    print("  â€¢ å¯ä»¥å¤„ç†å®ä½“é‡å å’ŒåµŒå¥—")
    print("  â€¢ ç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€pipeline")
    print("  â€¢ é€‚ç”¨äºå¤šç§ä¿¡æ¯æŠ½å–ä»»åŠ¡")

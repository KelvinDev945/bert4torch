# bert4torch å¼€å‘è¿›åº¦

## âœ… å·²å®Œæˆ

- [x] åˆ›å»º style_examples ç›®å½•å’Œä¸‰ç§é£æ ¼çš„ MultiHeadAttention ç¤ºä¾‹
- [x] ç­‰å¾…ç”¨æˆ·é€‰æ‹©ä»£ç é£æ ¼ï¼ˆé€‰æ‹©ï¼šé£æ ¼ A ç®€æ´é£æ ¼ï¼‰
- [x] æ­å»ºé¡¹ç›®åŸºç¡€ç»“æ„ï¼ˆç›®å½•ã€__init__.pyã€setup.pyï¼‰
- [x] å®ç° backend.pyï¼ˆä½ç½®ç¼–ç ã€maskã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
- [x] å®ç° layers.pyï¼ˆMultiHeadAttentionã€FeedForwardã€LayerNorm ç­‰ï¼‰
- [x] å®ç° models.py åŸºç¡€ï¼ˆBERTã€RoFormerã€GPTã€T5 æ¨¡å‹ï¼‰
- [x] å®ç° optimizers.pyï¼ˆAdamWã€å­¦ä¹ ç‡è°ƒåº¦ã€EMAã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼‰
- [x] å®ç° snippets.pyï¼ˆæ•°æ®å¤„ç†ã€è§£ç å™¨ç­‰å·¥å…·å‡½æ•°ï¼‰
- [x] å®ç° tokenizers.pyï¼ˆBERTåˆ†è¯å™¨ï¼‰

## ğŸš§ è¿›è¡Œä¸­

- [ ] ç¼–å†™ç¤ºä¾‹ä»£ç éªŒè¯åŠŸèƒ½

## ğŸ“‹ å¾…å®Œæˆ

- [ ] ç¼–å†™ç¤ºä¾‹ä»£ç 
  - [ ] åŸºç¡€ç¤ºä¾‹ï¼ˆç‰¹å¾æå–ã€MLMæµ‹è¯•ï¼‰
  - [ ] æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹
  - [ ] åºåˆ—æ ‡æ³¨ç¤ºä¾‹ï¼ˆNER + CRFï¼‰
  - [ ] æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼ˆGPTï¼‰
  - [ ] Seq2Seq ç¤ºä¾‹ï¼ˆT5ï¼‰
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] å®Œå–„æ–‡æ¡£

## ğŸ“ å¼€å‘æ—¥å¿—

### 2025-11-11

#### å®Œæˆçš„å·¥ä½œ

1. **é¡¹ç›®ç»“æ„æ­å»º**
   - åˆ›å»º `bert4torch/` ä¸»åŒ…ç›®å½•
   - åˆ›å»º `examples/`ã€`tests/` ç›®å½•
   - ç¼–å†™ `setup.py` å’Œ `README.md`

2. **backend.py å®ç°**
   - `gelu()`: GELU æ¿€æ´»å‡½æ•°
   - `sinusoidal_embeddings()`: æ­£å¼¦ä½ç½®ç¼–ç 
   - `apply_rotary_position_embeddings()`: RoPE æ—‹è½¬ä½ç½®ç¼–ç 
   - `sequence_masking()`: åºåˆ—maskæ“ä½œ
   - `attention_normalize()`: attentionå½’ä¸€åŒ–
   - `piecewise_linear()`: åˆ†æ®µçº¿æ€§å‡½æ•°ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ï¼‰

3. **layers.py å®ç°**
   - `MultiHeadAttention`: å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ”¯æŒäº¤å‰æ³¨æ„åŠ›ã€ä½ç½®åç½®ï¼‰
   - `FeedForward`: å‰é¦ˆç½‘ç»œ
   - `LayerNorm`: å±‚å½’ä¸€åŒ–ï¼ˆæ”¯æŒæ¡ä»¶LNï¼‰
   - `Embedding`: åµŒå…¥å±‚
   - `PositionEmbedding`: å¯å­¦ä¹ ä½ç½®ç¼–ç 
   - `SinusoidalPositionEmbedding`: æ­£å¼¦ä½ç½®ç¼–ç 
   - `RoPEPositionEmbedding`: æ—‹è½¬ä½ç½®ç¼–ç 
   - `RelativePositionEmbedding`: T5ç›¸å¯¹ä½ç½®ç¼–ç 
   - `GlobalPointer`: å…¨å±€æŒ‡é’ˆï¼ˆå®ä½“è¯†åˆ«ï¼‰
   - `CRF`: æ¡ä»¶éšæœºåœºï¼ˆåºåˆ—æ ‡æ³¨ï¼‰

4. **models.py å®ç°**
   - `Transformer`: åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
   - `BERT`: æ ‡å‡†BERTå®ç°ï¼ˆæ”¯æŒ MLMã€NSPã€poolerï¼‰
   - `BERTLayer`: BERT Transformerå±‚
   - `RoFormer`: å¸¦RoPEçš„BERT
   - `RoFormerLayer`: RoFormerå±‚
   - `GPT`: GPTå•å‘è¯­è¨€æ¨¡å‹
   - `GPTLayer`: GPT Transformerå±‚
   - `T5`: T5 Encoder-Decoderæ¨¡å‹
   - `T5Stack`: T5ç¼–ç å™¨/è§£ç å™¨æ ˆ
   - `T5Layer`: T5 Transformerå±‚
   - `build_transformer_model()`: ç»Ÿä¸€æ¨¡å‹æ„å»ºæ¥å£

#### ä»£ç ç‰¹ç‚¹

- é‡‡ç”¨**é£æ ¼ Aï¼ˆbert4keras ç®€æ´é£æ ¼ï¼‰**
- ä»£ç ç®€æ´ï¼Œå˜é‡å‘½åç®€çŸ­ï¼ˆq, k, v, oï¼‰
- æœ€å°‘çš„æ³¨é‡Šå’Œæ–‡æ¡£
- å•æ–‡ä»¶ç»„ç»‡ï¼Œä¾¿äºé˜…è¯»å’Œä¿®æ”¹

5. **optimizers.py å®ç°**
   - `AdamW`: AdamW ä¼˜åŒ–å™¨
   - `extend_with_weight_decay()`: æƒé‡è¡°å‡è£…é¥°å™¨
   - `extend_with_piecewise_linear_lr()`: åˆ†æ®µçº¿æ€§å­¦ä¹ ç‡è£…é¥°å™¨
   - `extend_with_gradient_accumulation()`: æ¢¯åº¦ç´¯ç§¯è£…é¥°å™¨
   - `extend_with_exponential_moving_average()`: EMA è£…é¥°å™¨
   - `extend_with_lookahead()`: Lookahead è£…é¥°å™¨
   - `get_linear_schedule_with_warmup()`: çº¿æ€§ warmup è°ƒåº¦å™¨
   - `get_cosine_schedule_with_warmup()`: ä½™å¼¦ warmup è°ƒåº¦å™¨

6. **snippets.py å®ç°**
   - `sequence_padding()`: åºåˆ—å¡«å……
   - `truncate_sequences()`: åºåˆ—æˆªæ–­
   - `text_segmentate()`: æ–‡æœ¬åˆ†æ®µ
   - `DataGenerator`: æ•°æ®ç”Ÿæˆå™¨åŸºç±»
   - `AutoRegressiveDecoder`: è‡ªå›å½’è§£ç å™¨ï¼ˆbeam searchã€random sampleï¼‰
   - `ViterbiDecoder`: ç»´ç‰¹æ¯”è§£ç å™¨
   - `parallel_apply()`: å¹¶è¡Œå¤„ç†
   - è£…é¥°å™¨å·¥å…·å‡½æ•°

7. **tokenizers.py å®ç°**
   - `TokenizerBase`: åˆ†è¯å™¨åŸºç±»
   - `Tokenizer`: BERT åˆ†è¯å™¨
   - `load_vocab()`: åŠ è½½è¯è¡¨
   - `save_vocab()`: ä¿å­˜è¯è¡¨
   - WordPiece åˆ†è¯
   - ä¸­æ–‡å­—ç¬¦å¤„ç†
   - `rematch()`: token æ˜ å°„å›åŸæ–‡æœ¬

#### ä¸‹ä¸€æ­¥è®¡åˆ’

1. ç¼–å†™ç¤ºä¾‹ä»£ç éªŒè¯åŠŸèƒ½
2. ç¼–å†™å•å…ƒæµ‹è¯•
3. å®Œå–„æ–‡æ¡£

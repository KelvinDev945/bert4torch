# bert4torch å¼€å‘è¿›åº¦

## âœ… å·²å®Œæˆ

- [x] åˆ›å»º style_examples ç›®å½•å’Œä¸‰ç§é£æ ¼çš„ MultiHeadAttention ç¤ºä¾‹
- [x] ç­‰å¾…ç”¨æˆ·é€‰æ‹©ä»£ç é£æ ¼ï¼ˆé€‰æ‹©ï¼šé£æ ¼ A ç®€æ´é£æ ¼ï¼‰
- [x] æ­å»ºé¡¹ç›®åŸºç¡€ç»“æ„ï¼ˆç›®å½•ã€__init__.pyã€setup.pyï¼‰
- [x] å®ç° backend.pyï¼ˆä½ç½®ç¼–ç ã€maskã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
- [x] å®ç° layers.pyï¼ˆMultiHeadAttentionã€FeedForwardã€LayerNorm ç­‰ï¼‰
- [x] å®ç° models.py åŸºç¡€ï¼ˆBERTã€RoFormerã€GPTã€T5 æ¨¡å‹ï¼‰

## ğŸš§ è¿›è¡Œä¸­

- [ ] å®ç°ä¼˜åŒ–å™¨å’Œè®­ç»ƒæŠ€å·§

## ğŸ“‹ å¾…å®Œæˆ

- [ ] å®ç°å·¥å…·å‡½æ•°ï¼ˆsnippets.pyï¼‰
- [ ] å®ç° tokenizers.py
- [ ] ç¼–å†™ç¤ºä¾‹ä»£ç 
  - [ ] åŸºç¡€ç¤ºä¾‹ï¼ˆç‰¹å¾æå–ã€MLMæµ‹è¯•ï¼‰
  - [ ] æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹
  - [ ] åºåˆ—æ ‡æ³¨ç¤ºä¾‹ï¼ˆNER + CRFï¼‰
  - [ ] æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼ˆGPTï¼‰
  - [ ] Seq2Seq ç¤ºä¾‹ï¼ˆT5ï¼‰
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] å®Œå–„æ–‡æ¡£

## ğŸ“ å¼€å‘æ—¥å¿—

### 2025-11-11

#### å®Œæˆçš„å·¥ä½œ

1. **é¡¹ç›®ç»“æ„æ­å»º**
   - åˆ›å»º `bert4torch/` ä¸»åŒ…ç›®å½•
   - åˆ›å»º `examples/`ã€`tests/` ç›®å½•
   - ç¼–å†™ `setup.py` å’Œ `README.md`

2. **backend.py å®ç°**
   - `gelu()`: GELU æ¿€æ´»å‡½æ•°
   - `sinusoidal_embeddings()`: æ­£å¼¦ä½ç½®ç¼–ç 
   - `apply_rotary_position_embeddings()`: RoPE æ—‹è½¬ä½ç½®ç¼–ç 
   - `sequence_masking()`: åºåˆ—maskæ“ä½œ
   - `attention_normalize()`: attentionå½’ä¸€åŒ–
   - `piecewise_linear()`: åˆ†æ®µçº¿æ€§å‡½æ•°ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ï¼‰

3. **layers.py å®ç°**
   - `MultiHeadAttention`: å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ”¯æŒäº¤å‰æ³¨æ„åŠ›ã€ä½ç½®åç½®ï¼‰
   - `FeedForward`: å‰é¦ˆç½‘ç»œ
   - `LayerNorm`: å±‚å½’ä¸€åŒ–ï¼ˆæ”¯æŒæ¡ä»¶LNï¼‰
   - `Embedding`: åµŒå…¥å±‚
   - `PositionEmbedding`: å¯å­¦ä¹ ä½ç½®ç¼–ç 
   - `SinusoidalPositionEmbedding`: æ­£å¼¦ä½ç½®ç¼–ç 
   - `RoPEPositionEmbedding`: æ—‹è½¬ä½ç½®ç¼–ç 
   - `RelativePositionEmbedding`: T5ç›¸å¯¹ä½ç½®ç¼–ç 
   - `GlobalPointer`: å…¨å±€æŒ‡é’ˆï¼ˆå®ä½“è¯†åˆ«ï¼‰
   - `CRF`: æ¡ä»¶éšæœºåœºï¼ˆåºåˆ—æ ‡æ³¨ï¼‰

4. **models.py å®ç°**
   - `Transformer`: åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
   - `BERT`: æ ‡å‡†BERTå®ç°ï¼ˆæ”¯æŒ MLMã€NSPã€poolerï¼‰
   - `BERTLayer`: BERT Transformerå±‚
   - `RoFormer`: å¸¦RoPEçš„BERT
   - `RoFormerLayer`: RoFormerå±‚
   - `GPT`: GPTå•å‘è¯­è¨€æ¨¡å‹
   - `GPTLayer`: GPT Transformerå±‚
   - `T5`: T5 Encoder-Decoderæ¨¡å‹
   - `T5Stack`: T5ç¼–ç å™¨/è§£ç å™¨æ ˆ
   - `T5Layer`: T5 Transformerå±‚
   - `build_transformer_model()`: ç»Ÿä¸€æ¨¡å‹æ„å»ºæ¥å£

#### ä»£ç ç‰¹ç‚¹

- é‡‡ç”¨**é£æ ¼ Aï¼ˆbert4keras ç®€æ´é£æ ¼ï¼‰**
- ä»£ç ç®€æ´ï¼Œå˜é‡å‘½åç®€çŸ­ï¼ˆq, k, v, oï¼‰
- æœ€å°‘çš„æ³¨é‡Šå’Œæ–‡æ¡£
- å•æ–‡ä»¶ç»„ç»‡ï¼Œä¾¿äºé˜…è¯»å’Œä¿®æ”¹

#### ä¸‹ä¸€æ­¥è®¡åˆ’

1. å®ç° `optimizers.py`ï¼ˆä¼˜åŒ–å™¨æ‰©å±•å’Œè®­ç»ƒæŠ€å·§ï¼‰
2. å®ç° `snippets.py`ï¼ˆæ•°æ®åŠ è½½ã€è§£ç å™¨ç­‰å·¥å…·å‡½æ•°ï¼‰
3. å®ç° `tokenizers.py`ï¼ˆåˆ†è¯å™¨ï¼‰
4. ç¼–å†™ç¤ºä¾‹ä»£ç éªŒè¯åŠŸèƒ½
